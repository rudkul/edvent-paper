{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur60gI8eI_oQ"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ28B7MqKWVz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lNcDAcp8MjE"
      },
      "outputs": [],
      "source": [
        "!pip install h5py\n",
        "!pip install typing-extensions\n",
        "!pip install wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbwiiaQv8NKA"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.27.0\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install diffusers==0.11.1\n",
        "!pip install transformers scipy ftfy accelerate\n",
        "!pip install langchain\n",
        "!pip install -q openai\n",
        "!pip install deep_translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHwVKa7SFb8N"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/9db565e15e8e35f1ebb48e1409b8ddb129f6f1e9/examples/dreambooth/train_dreambooth.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate==0.12.0 transformers ftfy bitsandbytes gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQV-cKO46jK2"
      },
      "outputs": [],
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq torch==2.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.34.1 --progress-bar off\n",
        "!pip install -qqq accelerate==0.23.0 --progress-bar off\n",
        "!pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n",
        "!pip install -qqq llava-torch==1.1.1 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5YrI7YG6m2H"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from io import BytesIO\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "from llava.constants import DEFAULT_IMAGE_TOKEN, IMAGE_TOKEN_INDEX\n",
        "from llava.conversation import SeparatorStyle, conv_templates\n",
        "from llava.mm_utils import (\n",
        "    KeywordsStoppingCriteria,\n",
        "    get_model_name_from_path,\n",
        "    process_images,\n",
        "    tokenizer_image_token,\n",
        ")\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.utils import disable_torch_init\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXj475RI6paL"
      },
      "outputs": [],
      "source": [
        "disable_torch_init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8amWBPL6q2N"
      },
      "outputs": [],
      "source": [
        "MODEL = \"4bit/llava-v1.5-13b-3GB\"\n",
        "model_name = get_model_name_from_path(MODEL)\n",
        "model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7QC3ozt8Od6"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet --upgrade diffusers transformers accelerate invisible_watermark mediapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7b6PrLQ69k0"
      },
      "outputs": [],
      "source": [
        "!pip install langchain gradio pypdf pandas matplotlib tiktoken textract transformers openai==0.28.1 faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJnIDwbn8P0d"
      },
      "outputs": [],
      "source": [
        "from deep_translator import (GoogleTranslator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ntyCuPe8RRk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "openai.api_key =  \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6GXlrBQJLJ-"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJZMymKe6_Ol"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IigEqlHD6swv"
      },
      "outputs": [],
      "source": [
        "CONV_MODE = \"llava_v0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZR-gAlk1p7T"
      },
      "outputs": [],
      "source": [
        "!pip install assemblyai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiNm6iOT1nsv"
      },
      "outputs": [],
      "source": [
        "import assemblyai as aai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Mgl5pPIa0j"
      },
      "outputs": [],
      "source": [
        "from nltk import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81K2fkFZIbVB"
      },
      "outputs": [],
      "source": [
        "!pip install python-pptx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzxYvTrsGFgi"
      },
      "outputs": [],
      "source": [
        "use_refiner = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J1HVvAkFh9Q"
      },
      "outputs": [],
      "source": [
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"CompVis/stable-diffusion-v1-4\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Path for images of the concept for training.\n",
        "INSTANCE_DIR = \"/content/data/sks\" #@param {type:\"string\"}\n",
        "!mkdir -p $INSTANCE_DIR\n",
        "\n",
        "#@markdown A general name for class like dog for dog images.\n",
        "CLASS_NAME = \"solar\" #@param {type:\"string\"}\n",
        "CLASS_DIR = f\"/content/data/{CLASS_NAME}\"\n",
        "\n",
        "#@markdown A rare token name which will reference the subject.\n",
        "TOKEN_NAME = \"sks\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = False #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"stable_diffusion_weights/sks\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "#@markdown sks is a rare identifier, feel free to replace it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Pn0XtqFlIW"
      },
      "outputs": [],
      "source": [
        "#@markdown Upload your images by running this cell.\n",
        "\n",
        "#@markdown OR\n",
        "\n",
        "#@markdown You can use the file manager on the left panel to upload (drag and drop) to INSTANCE_DIR (it uploads faster)\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    dst_path = os.path.join(INSTANCE_DIR, filename)\n",
        "    shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF3xgugMFnLy"
      },
      "outputs": [],
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --instance_data_dir=$INSTANCE_DIR \\\n",
        "  --class_data_dir=$CLASS_DIR \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --instance_prompt=\"photo of {TOKEN_NAME} {CLASS_NAME}\" \\\n",
        "  --class_prompt=\"photo of a {CLASS_NAME}\" \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=5e-6 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=50 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwHQQJJTFxmJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = OUTPUT_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to(\"cuda\")\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jawMrjqHGGDH"
      },
      "outputs": [],
      "source": [
        "import mediapy as media\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        "    )\n",
        "\n",
        "if use_refiner:\n",
        "  refiner = DiffusionPipeline.from_pretrained(\n",
        "      \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "      text_encoder_2=pipe.text_encoder_2,\n",
        "      vae=pipe.vae,\n",
        "      torch_dtype=torch.float16,\n",
        "      use_safetensors=True,\n",
        "      variant=\"fp16\",\n",
        "  )\n",
        "\n",
        "  refiner = refiner.to(\"cuda\")\n",
        "\n",
        "  pipe.enable_model_cpu_offload()\n",
        "else:\n",
        "  pipe = pipe.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-_cBRZc6sPK"
      },
      "outputs": [],
      "source": [
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path=MODEL, model_base=None, model_name=model_name, load_4bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJhToSjd8Xxt"
      },
      "outputs": [],
      "source": [
        "def add_newlines(filename):\n",
        "  with open(filename, 'r+') as file:\n",
        "      content = file.read()\n",
        "      content = content.replace('.', '.\\n')\n",
        "      file.seek(0)\n",
        "      file.write(content)\n",
        "      file.truncate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUBrrqRq-ekr"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.decomposition import NMF\n",
        "\n",
        "# # Load the text data from a file\n",
        "# def load_data(filename):\n",
        "#     with open(filename, 'r') as file:\n",
        "#         data = file.readlines()\n",
        "#     data = [sentence.strip() for sentence in data]\n",
        "#     return data\n",
        "\n",
        "# # Preprocess the data and extract features using TF-IDF\n",
        "# def preprocess_data(data):\n",
        "#     vectorizer = TfidfVectorizer(max_features=1000, lowercase=True, stop_words='english')\n",
        "#     tfidf_matrix = vectorizer.fit_transform(data)\n",
        "#     feature_names = vectorizer.get_feature_names_out()\n",
        "#     return tfidf_matrix, feature_names\n",
        "\n",
        "# # Perform topic modeling using NMF\n",
        "# def perform_topic_modeling(tfidf_matrix, num_topics):\n",
        "#     model = NMF(n_components=num_topics, random_state=42)\n",
        "#     model.fit(tfidf_matrix)\n",
        "#     return model\n",
        "\n",
        "# # Print the top words for each topic\n",
        "# def print_top_words(model, feature_names, num_top_words):\n",
        "#     my_dict1 = {}\n",
        "#     for topic_idx, topic in enumerate(model.components_):\n",
        "#         # print(\"Topic #%d:\" % topic_idx)\n",
        "#         topic_keywords = \" \".join([feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]])\n",
        "#         topic_keywords = topic_keywords.replace(\" \",\",\")\n",
        "#         topic_keywords1 = topic_keywords.split(\",\")\n",
        "#         # print(topic_keywords1)\n",
        "#         my_dict1[topic_keywords1[0]] = topic_keywords1\n",
        "#         # print()\n",
        "#     return my_dict1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5Am3Ji6fjmD"
      },
      "outputs": [],
      "source": [
        "def final_topic_dictionary(generated_content):\n",
        "    # Split the content into topics and keywords\n",
        "  content_parts = generated_content.split('\\n\\n')\n",
        "  split_text = [item.split('\\n') for item in content_parts]\n",
        "  flattened_text = [phrase for sublist in split_text for phrase in sublist]\n",
        "  # print(flattened_text)\n",
        "  # print(len(content_parts))\n",
        "\n",
        "  # Initialize an empty dictionary\n",
        "  result_dict = {}\n",
        "\n",
        "  # Iterate through the split text\n",
        "  for i in range(0, len(flattened_text), 2):\n",
        "      # Extract the number and keywords\n",
        "      topic_name = flattened_text[i]\n",
        "      keywords = flattened_text[i + 1].split('Keywords: ')[0]\n",
        "      print(keywords)\n",
        "      # Add to the dictionary\n",
        "      result_dict[topic_name] = keywords\n",
        "\n",
        "  return(result_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyzoFEzpfD8T"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import ast\n",
        "\n",
        "def topic_detection(text):\n",
        "  # res = openai.ChatCompletion.create(\n",
        "  #     model=\"gpt-3.5-turbo\",\n",
        "  #     max_tokens=1000,\n",
        "  #     temperature=0.7,\n",
        "  #     top_p=0.9,\n",
        "  #     frequency_penalty=0.5,\n",
        "  #     presence_penalty=1,\n",
        "  #     messages=[\n",
        "  #         {\n",
        "  #             \"role\": \"system\",\n",
        "  #             \"content\": \"You are a helpful assistant for topic detection.\",\n",
        "  #         },\n",
        "  #         {\n",
        "  #             \"role\": \"user\",\n",
        "  #             \"content\": f\"Give me 4 topics and its keywords in a dictionary format with the topic name in the following text: {text}\",\n",
        "  #         },\n",
        "  #     ],\n",
        "  # )\n",
        "\n",
        "  # # Extract the generated content from the response\n",
        "  # generated_content = res['choices'][0]['message']['content']\n",
        "  # return(generated_content)\n",
        "  # Use the loaded text in the request\n",
        "  res = openai.ChatCompletion.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      max_tokens=1000,\n",
        "      temperature=0.7,\n",
        "      top_p=0.9,\n",
        "      frequency_penalty=0.5,\n",
        "      presence_penalty=1,\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": \"You are a helpful assistant for topic detection.\",\n",
        "          },\n",
        "          {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Give me 4 topics and its 3 keywords in a dictionary format with the topic name as the key for the following text: {text}\n",
        "            The text is of educational matter which is used by the students of middle school. Each topic should have significant and correlated information about it in the text.\n",
        "            The text should be equally distributed under all topics.\n",
        "            The content of 4 topics together should makeup the text. \"\"\",\n",
        "        },\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  # Extract the generated content from the response\n",
        "  generated_content = res['choices'][0]['message']['content']\n",
        "  print(generated_content)\n",
        "\n",
        "  res = ast.literal_eval(generated_content)\n",
        "\n",
        "  # print result\n",
        "  print(res)\n",
        "  print(type(res))\n",
        "  return(res)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaZYwOMxDXxq"
      },
      "outputs": [],
      "source": [
        "def add_value(dict_obj, key, value):\n",
        "    ''' Adds a key-value pair to the dictionary.\n",
        "        If the key already exists in the dictionary,\n",
        "        it will associate multiple values with that\n",
        "        key instead of overwritting its value'''\n",
        "    if key not in dict_obj:\n",
        "        dict_obj[key] = [value]\n",
        "    else:\n",
        "      if isinstance(dict_obj[key], list):\n",
        "          dict_obj[key].append(value)\n",
        "      else:\n",
        "          dict_obj[key] = [dict_obj[key], value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U02IhJnkDYSK"
      },
      "outputs": [],
      "source": [
        "def group_sentences(sentences):\n",
        "    num_sentences = len(sentences)\n",
        "    group_size = 10\n",
        "    num_groups = num_sentences // group_size\n",
        "    remainder = num_sentences % group_size\n",
        "\n",
        "    grouped_sentences = [sentences[i * group_size: (i + 1) * group_size] for i in range(num_groups)]\n",
        "    if remainder > 0:\n",
        "        grouped_sentences.append(sentences[-remainder:])\n",
        "\n",
        "    return grouped_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpTaKPm_DemO",
        "outputId": "0731394a-417a-431c-a829-819e9c3f9a68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "def segregate_sentences(text_corpus, topic_keywords):\n",
        "    # Tokenize the text corpus into sentences\n",
        "    sentences = sent_tokenize(text_corpus)\n",
        "\n",
        "    # Create a dictionary to store the segregated sentences\n",
        "    segregated_sentences = {topic: [] for topic in topic_keywords}\n",
        "\n",
        "    # Iterate through each sentence\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence into words\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        # Check which topic the sentence relates to the most\n",
        "        max_match = 0\n",
        "        best_topic = None\n",
        "        for topic, keywords in topic_keywords.items():\n",
        "            match_count = sum(word in keywords for word in words)\n",
        "            if match_count > max_match:\n",
        "                max_match = match_count\n",
        "                best_topic = topic\n",
        "\n",
        "        # Add the sentence to the corresponding topic or assign it to the most appropriate one\n",
        "        if best_topic is not None:\n",
        "            segregated_sentences[best_topic].append(sentence)\n",
        "        else:\n",
        "            # Assign the sentence to the topic with the maximum matching keywords\n",
        "            keyword_matches = {topic: sum(word in keywords for word in words) for topic, keywords in topic_keywords.items()}\n",
        "            max_match_count = max(keyword_matches.values())\n",
        "            best_topics = [topic for topic, count in keyword_matches.items() if count == max_match_count]\n",
        "            segregated_sentences[best_topics[0]].append(sentence)\n",
        "\n",
        "    return segregated_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYXrPpTUGLTh"
      },
      "outputs": [],
      "source": [
        "def create_image(text):\n",
        "  prompt = text\n",
        "  seed = random.randint(0, sys.maxsize)\n",
        "\n",
        "  images = pipe(\n",
        "      prompt = prompt,\n",
        "      output_type = \"latent\" if use_refiner else \"pil\",\n",
        "      generator = torch.Generator(\"cuda\").manual_seed(seed),\n",
        "      ).images\n",
        "\n",
        "  if use_refiner:\n",
        "    images = refiner(\n",
        "        prompt = prompt,\n",
        "        image = images,\n",
        "        ).images\n",
        "\n",
        "  # print(f\"Prompt:\\t{prompt}\\nSeed:\\t{seed}\")\n",
        "  media.show_images(images)\n",
        "  images[0].save(\"image.jpg\")\n",
        "  return images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNwiYa9VGOUN"
      },
      "outputs": [],
      "source": [
        "def translate_to_kan(text):\n",
        "  translated_kan = GoogleTranslator(source='auto', target='kannada').translate(text=text)\n",
        "  return translated_kan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohC5tBQZGOv3"
      },
      "outputs": [],
      "source": [
        "def translate_to_hin(text):\n",
        "  translated_hin = GoogleTranslator(source='auto', target='hindi').translate(text=text)\n",
        "  return translated_hin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MziXRgbWILlZ",
        "outputId": "9bb67d67-05d4-4272-a4ad-c2889880359b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import collections\n",
        "import collections.abc\n",
        "from pptx import Presentation\n",
        "from pptx.util import Pt\n",
        "from pptx.util import Inches\n",
        "import math\n",
        "def final_ppt_step_english(cleaned_data):\n",
        "  prs = Presentation()\n",
        "  bullet_slide_layout = prs.slide_layouts[3]\n",
        "  img_path = '/content/Logo.png'\n",
        "\n",
        "\n",
        "  for topic, summaries in cleaned_data.items():\n",
        "    for summary in summaries:\n",
        "      k = 1\n",
        "      summary_list = sent_tokenize(summary)\n",
        "      slide = prs.slides.add_slide(bullet_slide_layout)\n",
        "      shapes = slide.shapes\n",
        "      title_shape = shapes.title\n",
        "      body_shape = shapes.placeholders[1]\n",
        "      title_shape.text = topic.capitalize()\n",
        "      pic = slide.shapes.add_picture(img_path, Inches(8.5), Inches(0.3))\n",
        "      for text in summary_list:\n",
        "        if k%4 == 0:\n",
        "          slide = prs.slides.add_slide(bullet_slide_layout)\n",
        "          shapes = slide.shapes\n",
        "          title_shape = shapes.title\n",
        "          body_shape = shapes.placeholders[1]\n",
        "          title_shape.text = topic.capitalize()\n",
        "          pic = slide.shapes.add_picture(img_path, Inches(8.5), Inches(0.3))\n",
        "        p = body_shape.text_frame.add_paragraph()\n",
        "        p.font.size = Pt(23)\n",
        "        p.font.name = 'Nunito'\n",
        "        p.text = text.strip().capitalize()\n",
        "        if k == 1 or k%4 == 0:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.18, Inches(3.5), Inches(3))\n",
        "        elif k == 2:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.59, Inches(3.5), Inches(3))\n",
        "        else:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.59, Inches(3.5), Inches(3))\n",
        "        p.level = 0\n",
        "        k = k + 1\n",
        "  pptx_file_path = 'Demo.pptx'\n",
        "  prs.save(pptx_file_path)\n",
        "  return pptx_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRB6Gaam9dgc",
        "outputId": "72b90f74-3203-48f5-838b-faa57e0bfd48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import collections\n",
        "import collections.abc\n",
        "from pptx import Presentation\n",
        "from pptx.util import Pt\n",
        "from pptx.util import Inches\n",
        "import math\n",
        "def final_ppt_step_kannada(cleaned_data):\n",
        "  prs = Presentation()\n",
        "  bullet_slide_layout = prs.slide_layouts[3]\n",
        "  img_path = '/content/Logo.png'\n",
        "\n",
        "\n",
        "  for topic, summaries in cleaned_data.items():\n",
        "    for summary in summaries:\n",
        "      k = 1\n",
        "      summary_list = sent_tokenize(summary)\n",
        "      slide = prs.slides.add_slide(bullet_slide_layout)\n",
        "      shapes = slide.shapes\n",
        "      title_shape = shapes.title\n",
        "      body_shape = shapes.placeholders[1]\n",
        "      title_shape.text = translate_to_kan(topic.capitalize())\n",
        "      pic = slide.shapes.add_picture(img_path, Inches(8.5), Inches(0.3))\n",
        "      for text in summary_list:\n",
        "        if k%4 == 0:\n",
        "          slide = prs.slides.add_slide(bullet_slide_layout)\n",
        "          shapes = slide.shapes\n",
        "          title_shape = shapes.title\n",
        "          body_shape = shapes.placeholders[1]\n",
        "          title_shape.text = translate_to_kan(topic.capitalize())\n",
        "          pic = slide.shapes.add_picture(img_path, Inches(8.5), Inches(0.3))\n",
        "        p = body_shape.text_frame.add_paragraph()\n",
        "        p.font.size = Pt(23)\n",
        "        p.font.name = 'Nunito'\n",
        "        p.text = translate_to_kan(text.strip().capitalize())\n",
        "        if k == 1 or k%4 == 0:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.18, Inches(3.5), Inches(3))\n",
        "        elif k == 2:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.59, Inches(3.5), Inches(3))\n",
        "        else:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.59, Inches(3.5), Inches(3))\n",
        "        p.level = 0\n",
        "        k = k + 1\n",
        "  pptx_file_path = 'Demo.pptx'\n",
        "  prs.save(pptx_file_path)\n",
        "  return pptx_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-oh5Xh99tIi",
        "outputId": "c3ac9f1b-4f77-4ec9-ca7a-5a9dc81bbb2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import collections\n",
        "import collections.abc\n",
        "from pptx import Presentation\n",
        "from pptx.util import Pt\n",
        "from pptx.util import Inches\n",
        "import math\n",
        "def final_ppt_step_hindi(cleaned_data):\n",
        "  prs = Presentation()\n",
        "  bullet_slide_layout = prs.slide_layouts[3]\n",
        "  img_path = '/content/Logo.png'\n",
        "\n",
        "\n",
        "  for topic, summaries in cleaned_data.items():\n",
        "    for summary in summaries:\n",
        "      k = 1\n",
        "      summary_list = sent_tokenize(summary)\n",
        "      slide = prs.slides.add_slide(bullet_slide_layout)\n",
        "      shapes = slide.shapes\n",
        "      title_shape = shapes.title\n",
        "      body_shape = shapes.placeholders[1]\n",
        "      title_shape.text = translate_to_hin(topic.capitalize())\n",
        "      pic = slide.shapes.add_picture(img_path, Inches(8.5), Inches(0.3))\n",
        "      for text in summary_list:\n",
        "        if k%4 == 0:\n",
        "          slide = prs.slides.add_slide(bullet_slide_layout)\n",
        "          shapes = slide.shapes\n",
        "          title_shape = shapes.title\n",
        "          body_shape = shapes.placeholders[1]\n",
        "          title_shape.text = translate_to_hin(topic.capitalize())\n",
        "          pic = slide.shapes.add_picture(img_path, Inches(8.5), Inches(0.3))\n",
        "        p = body_shape.text_frame.add_paragraph()\n",
        "        p.font.size = Pt(23)\n",
        "        p.font.name = 'Nunito'\n",
        "        p.text = translate_to_hin(text.strip().capitalize())\n",
        "        if k == 1 or k%4 == 0:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.18, Inches(3.5), Inches(3))\n",
        "        elif k == 2:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.59, Inches(3.5), Inches(3))\n",
        "        else:\n",
        "          generated_image = create_image(\"an image of\" + text.strip().capitalize())\n",
        "          pic1 = slide.shapes.add_picture(\"image.jpg\", prs.slide_width - Inches(4), prs.slide_height * 0.59, Inches(3.5), Inches(3))\n",
        "        p.level = 0\n",
        "        k = k + 1\n",
        "  pptx_file_path = 'Demo.pptx'\n",
        "  prs.save(pptx_file_path)\n",
        "  return pptx_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HqPHd1N8YpC"
      },
      "outputs": [],
      "source": [
        "def ppt_generation_english(file_name):\n",
        "  # opening the file in read mode\n",
        "  my_file = open(file_name, \"r\")\n",
        "  # reading the file\n",
        "  data_text = my_file.read()\n",
        "  add_newlines(file_name)\n",
        "\n",
        "  # data = load_data(file_name)\n",
        "  # tfidf_matrix, feature_names = preprocess_data(data)\n",
        "  # num_topics = 3  # Specify the number of topics\n",
        "  # model = perform_topic_modeling(tfidf_matrix, num_topics)\n",
        "  # num_top_words = 4  # Specify the number of top words to display for each topic\n",
        "\n",
        "  my_dict = topic_detection(data_text)\n",
        "  topic_sen = {}\n",
        "  text_corpus = data_text\n",
        "  topic_keywords = my_dict\n",
        "  segregated = segregate_sentences(text_corpus, topic_keywords)\n",
        "  for topic, sentences in segregated.items():\n",
        "      print(topic)\n",
        "      print(sentences)\n",
        "      length = len(sentences)\n",
        "      print(length)\n",
        "      if length>=20 and length <=30:\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[0:int(length/2)]))\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[int(length/2):int(length)]))\n",
        "      elif length > 30:\n",
        "        answer = group_sentences(sentences)\n",
        "        for i in range(0,len(answer)):\n",
        "          add_value(topic_sen, topic, ' '.join(answer[i]))\n",
        "      else:\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[0:int(length)]))\n",
        "  final_dict = {}\n",
        "  for topic, paragraphs in topic_sen.items():\n",
        "    for i in range (0,len(topic_sen[topic])):\n",
        "      summary_list =[]\n",
        "      text = paragraphs[i]\n",
        "      # prompt = \"You must have seen some of the landform features. Some parts of the lithosphere may be rugged and some flat. You will be amazed to know that the ground you are standing on is slowly moving within the Earth. A continuous movement is taking place. Broadly, we can group different landforms depending on elevation and slope as mountains, plateaus and plains. The mountains may have a small summit and a broad base. It is considerably higher than the surrounding area. Some mountains are even higher than the clouds. As you go higher, the climate becomes colder. In some mountains, there are permanently frozen rivers of ice.\"\n",
        "      res = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          max_tokens=1000,\n",
        "          temperature=0.7,\n",
        "          top_p=0.9,\n",
        "          frequency_penalty=0.5,\n",
        "          presence_penalty=1,\n",
        "          messages=[\n",
        "              {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant for text summarization.\",\n",
        "              },\n",
        "              {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Summarize this for a 6th grade student: {text}\",\n",
        "              },\n",
        "          ],\n",
        "      )\n",
        "      # print(\"Text:\", text)\n",
        "      summary_text = res[\"choices\"][0][\"message\"][\"content\"]\n",
        "      summary_list.append(summary_text)\n",
        "      # print(\"Summary:\", summary_text)\n",
        "      add_value(final_dict, topic, summary_text)\n",
        "  # print(final_dict)\n",
        "  cleaned_data = {}\n",
        "  for key, values in final_dict.items():\n",
        "      cleaned_values = [value.replace('\\n\\n', '') for value in values]\n",
        "      cleaned_data[key] = cleaned_values\n",
        "  # print(cleaned_data)\n",
        "\n",
        "  return (final_ppt_step_english(cleaned_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxUH0wBA96nv"
      },
      "outputs": [],
      "source": [
        "def ppt_generation_kannada(file_name):\n",
        "  # opening the file in read mode\n",
        "  my_file = open(file_name, \"r\")\n",
        "  # reading the file\n",
        "  data_text = my_file.read()\n",
        "  add_newlines(file_name)\n",
        "\n",
        "  # data = load_data(file_name)\n",
        "  # tfidf_matrix, feature_names = preprocess_data(data)\n",
        "  # num_topics = 3  # Specify the number of topics\n",
        "  # model = perform_topic_modeling(tfidf_matrix, num_topics)\n",
        "  # num_top_words = 4  # Specify the number of top words to display for each topic\n",
        "\n",
        "  my_dict = topic_detection(data_text)\n",
        "  topic_sen = {}\n",
        "  text_corpus = data_text\n",
        "  topic_keywords = my_dict\n",
        "  segregated = segregate_sentences(text_corpus, topic_keywords)\n",
        "  for topic, sentences in segregated.items():\n",
        "      print(topic)\n",
        "      print(sentences)\n",
        "      length = len(sentences)\n",
        "      print(length)\n",
        "      if length>=20 and length <=30:\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[0:int(length/2)]))\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[int(length/2):int(length)]))\n",
        "      elif length > 30:\n",
        "        answer = group_sentences(sentences)\n",
        "        for i in range(0,len(answer)):\n",
        "          add_value(topic_sen, topic, ' '.join(answer[i]))\n",
        "      else:\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[0:int(length)]))\n",
        "  final_dict = {}\n",
        "  for topic, paragraphs in topic_sen.items():\n",
        "    for i in range (0,len(topic_sen[topic])):\n",
        "      summary_list =[]\n",
        "      text = paragraphs[i]\n",
        "      # prompt = \"You must have seen some of the landform features. Some parts of the lithosphere may be rugged and some flat. You will be amazed to know that the ground you are standing on is slowly moving within the Earth. A continuous movement is taking place. Broadly, we can group different landforms depending on elevation and slope as mountains, plateaus and plains. The mountains may have a small summit and a broad base. It is considerably higher than the surrounding area. Some mountains are even higher than the clouds. As you go higher, the climate becomes colder. In some mountains, there are permanently frozen rivers of ice.\"\n",
        "      res = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          max_tokens=1000,\n",
        "          temperature=0.7,\n",
        "          top_p=0.9,\n",
        "          frequency_penalty=0.5,\n",
        "          presence_penalty=1,\n",
        "          messages=[\n",
        "              {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant for text summarization.\",\n",
        "              },\n",
        "              {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Summarize this for a 6th grade student: {text}\",\n",
        "              },\n",
        "          ],\n",
        "      )\n",
        "      # print(\"Text:\", text)\n",
        "      summary_text = res[\"choices\"][0][\"message\"][\"content\"]\n",
        "      summary_list.append(summary_text)\n",
        "      # print(\"Summary:\", summary_text)\n",
        "      add_value(final_dict, topic, summary_text)\n",
        "  # print(final_dict)\n",
        "  cleaned_data = {}\n",
        "  for key, values in final_dict.items():\n",
        "      cleaned_values = [value.replace('\\n\\n', '') for value in values]\n",
        "      cleaned_data[key] = cleaned_values\n",
        "  # print(cleaned_data)\n",
        "\n",
        "  return (final_ppt_step_kannada(cleaned_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnzkeT5D9-Wa"
      },
      "outputs": [],
      "source": [
        "def ppt_generation_hindi(file_name):\n",
        "   # opening the file in read mode\n",
        "  my_file = open(file_name, \"r\")\n",
        "  # reading the file\n",
        "  data_text = my_file.read()\n",
        "  add_newlines(file_name)\n",
        "\n",
        "  # data = load_data(file_name)\n",
        "  # tfidf_matrix, feature_names = preprocess_data(data)\n",
        "  # num_topics = 3  # Specify the number of topics\n",
        "  # model = perform_topic_modeling(tfidf_matrix, num_topics)\n",
        "  # num_top_words = 4  # Specify the number of top words to display for each topic\n",
        "\n",
        "  my_dict = topic_detection(data_text)\n",
        "  topic_sen = {}\n",
        "  text_corpus = data_text\n",
        "  topic_keywords = my_dict\n",
        "  segregated = segregate_sentences(text_corpus, topic_keywords)\n",
        "  for topic, sentences in segregated.items():\n",
        "      print(topic)\n",
        "      print(sentences)\n",
        "      length = len(sentences)\n",
        "      print(length)\n",
        "      if length>=20 and length <=30:\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[0:int(length/2)]))\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[int(length/2):int(length)]))\n",
        "      elif length > 30:\n",
        "        answer = group_sentences(sentences)\n",
        "        for i in range(0,len(answer)):\n",
        "          add_value(topic_sen, topic, ' '.join(answer[i]))\n",
        "      else:\n",
        "        add_value(topic_sen, topic, ' '.join(sentences[0:int(length)]))\n",
        "  final_dict = {}\n",
        "  for topic, paragraphs in topic_sen.items():\n",
        "    for i in range (0,len(topic_sen[topic])):\n",
        "      summary_list =[]\n",
        "      text = paragraphs[i]\n",
        "      # prompt = \"You must have seen some of the landform features. Some parts of the lithosphere may be rugged and some flat. You will be amazed to know that the ground you are standing on is slowly moving within the Earth. A continuous movement is taking place. Broadly, we can group different landforms depending on elevation and slope as mountains, plateaus and plains. The mountains may have a small summit and a broad base. It is considerably higher than the surrounding area. Some mountains are even higher than the clouds. As you go higher, the climate becomes colder. In some mountains, there are permanently frozen rivers of ice.\"\n",
        "      res = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          max_tokens=1000,\n",
        "          temperature=0.7,\n",
        "          top_p=0.9,\n",
        "          frequency_penalty=0.5,\n",
        "          presence_penalty=1,\n",
        "          messages=[\n",
        "              {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant for text summarization.\",\n",
        "              },\n",
        "              {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Summarize this for a 6th grade student: {text}\",\n",
        "              },\n",
        "          ],\n",
        "      )\n",
        "      # print(\"Text:\", text)\n",
        "      summary_text = res[\"choices\"][0][\"message\"][\"content\"]\n",
        "      summary_list.append(summary_text)\n",
        "      # print(\"Summary:\", summary_text)\n",
        "      add_value(final_dict, topic, summary_text)\n",
        "  # print(final_dict)\n",
        "  cleaned_data = {}\n",
        "  for key, values in final_dict.items():\n",
        "      cleaned_values = [value.replace('\\n\\n', '') for value in values]\n",
        "      cleaned_data[key] = cleaned_values\n",
        "  # print(cleaned_data)\n",
        "\n",
        "  return (final_ppt_step_hindi(cleaned_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLg_v3m61spt"
      },
      "outputs": [],
      "source": [
        "def transcribe_english(audio):\n",
        "  aai.settings.api_key = f\"1c5999c94b1640a58296640b256e4034\"\n",
        "  FILE_URL = audio\n",
        "  transcriber = aai.Transcriber()\n",
        "  transcript = transcriber.transcribe(FILE_URL)\n",
        "  with open('Text.txt', 'w') as f:\n",
        "    f.write(transcript.text)\n",
        "  return(ppt_generation_english('Text.txt'))\n",
        "  # return(transcript.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy0kMHSF-VSu"
      },
      "outputs": [],
      "source": [
        "def transcribe_kannada(audio):\n",
        "  aai.settings.api_key = f\"1c5999c94b1640a58296640b256e4034\"\n",
        "  FILE_URL = audio\n",
        "  transcriber = aai.Transcriber()\n",
        "  transcript = transcriber.transcribe(FILE_URL)\n",
        "  with open('Text.txt', 'w') as f:\n",
        "    f.write(transcript.text)\n",
        "  return(ppt_generation_kannada('Text.txt'))\n",
        "  # return(transcript.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RArGP-F-WYI"
      },
      "outputs": [],
      "source": [
        "def transcribe_hindi(audio):\n",
        "  aai.settings.api_key = f\"1c5999c94b1640a58296640b256e4034\"\n",
        "  FILE_URL = audio\n",
        "  transcriber = aai.Transcriber()\n",
        "  transcript = transcriber.transcribe(FILE_URL)\n",
        "  with open('Text.txt', 'w') as f:\n",
        "    f.write(transcript.text)\n",
        "  return(ppt_generation_hindi('Text.txt'))\n",
        "  # return(transcript.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mA1sQOpDhkE-"
      },
      "outputs": [],
      "source": [
        "tts_demo = gr.Interface(\n",
        "    fn = transcribe_english,\n",
        "    inputs = \"text\",\n",
        "    outputs = gr.File(),\n",
        "    title=None,\n",
        "    description=\"Upload your audio and generate the presentation in English!\",\n",
        "    cache_examples=False\n",
        ")\n",
        "tts_demo1 = gr.Interface(\n",
        "    fn = transcribe_kannada,\n",
        "    inputs = \"text\",\n",
        "    outputs = gr.File(),\n",
        "    title=None,\n",
        "    description=\"Upload your audio and generate the presentation in Kannada!\",\n",
        "    cache_examples=False\n",
        ")\n",
        "\n",
        "tts_demo2 = gr.Interface(\n",
        "    fn = transcribe_hindi,\n",
        "    inputs = \"text\",\n",
        "    outputs = gr.File(),\n",
        "    title=None,\n",
        "    description=\"Upload your audio and generate the presentation in Hindi!\",\n",
        "    cache_examples=False\n",
        ")\n",
        "\n",
        "demo = gr.TabbedInterface([tts_demo, tts_demo1, tts_demo2], [\"English Presentation\", \"Kannada Presentation\", \"Hindi Presentation\"], title=\"Edvent\", theme=gr.themes.Soft())\n",
        "demo.launch(debug = True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}